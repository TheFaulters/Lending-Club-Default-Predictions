{
  "name": "Lending Club default predictions",
  "tagline": "A chronicle of our analysis of Lending Club data.",
  "body": "# Lending Club Default Predictions\r\n_Arthur Aguillar, Dwijo Goswami and James Fallon_  \r\n_Harvard University_   \r\n_CS109A: Introduction to Data Science_  \r\n_Final project_  \r\n\r\n## Links to iPython notebooks\r\nForthcoming  \r\n\r\n## Data exploration\r\n\r\nOur task was to predict loan defaults using data provided by [Lending Club](https://www.lendingclub.com), a peer to peer lending service. The organization makes de-identified loan [data](https://www.lendingclub.com/info/download-data.action) available publicly. \r\n\r\nWe began by exploring the loan data. We categorized loans into those currently in delinquency or default (risky loans) and those with no payment violations (not risky loans).  \r\n   \r\nInitially, we wanted to explore some basic relationships. In the first instance, we were interested in how poorly loans had performed based on when they were issued.  \r\n\r\n**Figure 1:**  \r\n \r\n![](https://thefaulters.github.io/Lending-Club-Default-Predictions/Images/Loans-delinquency-bar.png)\r\n\r\nAdditionally, we were interested in whether higher risk borrowers (those with lower grades) consistently had to pay higher interest rates (and whether they indeed defaulted more often). We also wanted to make sure to account for imbalance in the dataset - since most borrowers don't default.  \r\n\r\n**Figure 2:**\r\n\r\n![](https://thefaulters.github.io/Lending-Club-Default-Predictions/Images/Risk.png) ![](https://thefaulters.github.io/Lending-Club-Default-Predictions/Images/Imbalance.png)\r\n\r\n## Cleaning  \r\nUsing all data from 2007 through the second quarter of 2016 resulted in a dataset with 1,119,194 loan observations. The dataset had 111 variables - pieces of information on each borrower ranging from the interest rate on the loan to a personal description or sales pitch describing the purpose of the loan.    \r\n\r\nIt is clear from Figure 1 above that risky loan rates decrease dramatically for loans that have been active for a shorter period of time. We feared that this could be due to the short duration of those loans relative to their maturity, and that using these to build a predictive model would result in identifying risky borrowers as not risky borrowers. So we dropped all 2016 data, using only data from 2007 through 2015.\r\n\r\nTo run most of our models, we needed the data to be in numerical format. However, the dataset was full of various datatypes. So our cleaning process mostly consisted of ensuring all numbers were indeed in numerical format, and generating dummy variables for categorical variables like loan grade and state of residence. Following feedback from our TF, we also dropped variables that could only be determined after the loan was issued. For example, delayed payments or late fees.   \r\n\r\nAfter the cleaning process, the final dataset contained 887,440 observations, 216 predictor variables and one binary response variable: a 1 for risky loans and a 0 for safe loans.  \r\n\r\n**Figure 3: Cleaning**    \r\n![](https://thefaulters.github.io/Lending-Club-Default-Predictions/Images/cleaning_1.png)![](https://thefaulters.github.io/Lending-Club-Default-Predictions/Images/cleaning_2.png)  \r\n![](https://thefaulters.github.io/Lending-Club-Default-Predictions/Images/cleaning_3.png)![](https://thefaulters.github.io/Lending-Club-Default-Predictions/Images/cleaning_4.png)  \r\n\r\n## Modelling  \r\n\r\nOnce we cleaned and explored the data, we began testing different models for accuracy in prediction. Our general approach towards testing a model was to use a small sub-sample for optimizing the parameters of the model, and then using these parameters to fit the model on a larger sub-set of the data. The sizes of sub-samples and sub-sets varied in some cases such as Support Vector Classification, due to computational efficiency constraints. We followed this approach to test three different models for binary classification - Random Forests, Logistic Regression and Support Vector Classification.\r\n\r\n## Random forests   \r\n\r\nRandom Forests is described as an 'ensemble' learning method for classification. Such a learning method runs multiple learning algorithms in order to improve predictive performance over a singular prediction (Rokach). The model generates randomized decision trees, and aggregates the mode prediction across these trees for classification problems. The randomization of the trees reduces the possibilities of overfitting the prediction to the training data.\r\n\r\nRandom Forests was the basis for our baseline numbers during Milestone 4. However, the results in the final project submission are not strictly comparable to Milestone 4 results, as we made a number of changes to our data and model based on feedback. Most prominently, we removed a number of predictor variables which were highly correlated with the response due to being gathered after the loan was issued. Likewise, we paid more attention to balancing class weights to compensate for the imbalanced data.\r\n\r\nDespite these changes, our original arguments for using Random Forests in Milestone 4 are still applicable. The model is very efficient (compared to the other models we tested), and has had prior success in predicting loan defaults in P2P platforms (Pandey). The model is also particularly good at handling high dimensionality, as it treats variables in a sequential manner as it goes through the trees.\r\n\r\nIn our modeling we first optimized Random Forest for number of trees and depth of the trees. We then fitted a larger sub-sample for the most predictive number of trees and depth size. As with all out models, we balanced the class weights to compensate for the unbalanced dataset shown above.  \r\n\r\n**Figure 4:**  \r\n\r\n![](https://thefaulters.github.io/Lending-Club-Default-Predictions/Images/RF_opt.png)\r\n\r\n## Logistic regression     \r\n\r\nLogistic Regression is the first model that comes to mind in terms of binary classification. It is a probabilistic model for categorical dependent variables i.e. it uses the input data to predicts the probability of the output category (James et al p 131). This model tends to perform better in datasets where the classification problem is linearly separable. Logistic Regression also offers us the opportunity to penalize coefficient predictions.  The L1 and L2 penalties also provide us room to do feature selection and compensate for the high dimensionality of any dataset.\r\n\r\nThe approach to optimizing the Logistic Regression model was as follows. We first optimized for the penalty parameter using 3 fold cross validation, for both l1 and l2 penalties. Once we had the optimal parameter for each penalty function, we fitted the data on a larger subsample to test for accuracy. The l1 penalty consistently outperformed the l2 penalty. However, in the case of our dataset, we did not find Logistic Regression to perform better than Random Forests either in terms of accuracy or computational efficiency.   \r\n\r\n**Figure 5: l1 optimization**  \r\n![](https://thefaulters.github.io/Lending-Club-Default-Predictions/Images/l1_opt.png)  \r\n\r\n**Figure 6: l2 optimization**  \r\n![](https://thefaulters.github.io/Lending-Club-Default-Predictions/Images/l2_opt.png)\r\n\r\n## Support Vector Classifier  \r\n\r\nNext, we used a support vector machine to classify loan risk. We chose SVM because our dataset is very large and features a binary response variable. Our approach was guided by James et al pp 344-35 and Hsu et al.  \r\n\r\nWhile we had expected SVM to be more efficient than other models, it was in fact one of the slowest models we attempted. As a result, we tuned with 1% of the dataset (8,874 observations), trained with 5% and validated with 5% (44,372 observations).   \r\n\r\nOn the guidance of Hsu et al, we tuned for gamma - a parameter specific to the radial kernel - and C - the misclassification parameter. We also scaled the x input to have mean 0 and standard deviation 1. As with our other models, we balanced the class weights to account for the underrepresentation of risky loans in the overall dataset.   \r\n\r\nThe performance overall of SVM was good, but not better than Random Forests (see results below). Importantly, Random Forests outperformed SVM from a computational perspective - we were unable to use the full dataset for training and testing SVM.  \r\n\r\n**Figure 7:**\r\n![](https://thefaulters.github.io/Lending-Club-Default-Predictions/Images/svm_opt.png)  \r\n\r\n# Results \r\n\r\nThe most important feedback we received from Milestone 4 was to focus less on the overall accuracy of the prediction and more on the ability of our model to predict loan risk. Ultimately, we care about our models' ability to predict a risky loan, which will always be a smaller number of cases in a healthy loan portfolio.\r\n\r\nTherefore, in our final results and model selection, we have compared models not on their overall accuracy, but on their ability to predict loan risk. We used two mechanisms to compare their relative performance - a Receiver Operating Curve (ROC) and a Precision/Recall curve.\r\n\r\nA ROC is perhaps the most intuitive to understand. It represents the trade off between a true positive and a false positive prediction (James et al p 147). As any model attempts to identify more 'true' risky loans, it is likely to identify a number of non-risky loans as risky as collateral damage for spreading its net too far. The lower the collateral damage, the better the model. Therefore one can use ROC to understand how efficiently any model is identifying defaults correctly. Most intuitively, this can be understood as the area under the ROC curve. The higher the area under the curve, the more efficiently the model is making the ROC tradeoff. For ROC, the more accurate models are closer to the upper left of the graph.\r\n\r\nA Precision/Recall Curve is slightly more complicated. The Precision Rate of any model is the proportion of 'true' risky loans included in its 'true' predictions. The Recall Rate on the other hand is the proportion of correctly identified risky loans to all the 'true' risky predictions. Much like the ROC, the curve represents a trade off - as a model attempts to identify more of the 'true' risky loans (i.e. increase the Recall Rate), its Precision Rate will decline due to collateral damage (Davis and Goodrich). Again, the model which demonstrates the lowest trade off between Precision and Recall is the most effective model.  For Precision/Recall, the more accurate models are closer to the upper right of the graph.  \r\n  \r\n**Figure 8:**  \r\n![](https://thefaulters.github.io/Lending-Club-Default-Predictions/Images/ROC.png)![](https://thefaulters.github.io/Lending-Club-Default-Predictions/Images/Penalty_Recall.png)  \r\n\r\nIn all of our assessments, we consistently found that Random Forests outperformed all other models in terms of accuracy of prediction for the risky class.  \r\n\r\nWhen we finally tested our optimized model of choice, we found it predicts the risk status of loans with 93.5% accuracy overall in a testing set equivalent to 60% of our full dataset (over 500,000 loans). The risky class accuracy performance is less impressive. The model accurately predicts 42.62% of true risky lenders observed in this test set.  \r\n\r\n## Appendix: Sentiment analysis\r\n\r\nWhile we had initially dropped the borrower's description, we wanted to revisit this variable to conduct sentiment analysis. Essentially, we hoped to use the borrower's loan pitch to see if descriptions contained any particular red flags.  \r\n\r\nWe first performed a visual analysis of the words used more often by risky borrowers and non risky borrowers. It is interesting to observe that the word \"card\" is the most used more frequently by risky borrowers, while the word loan is the most used relatively more frequently by non risky borrowers: One possible intuition for this is that risky borrowers are often refinancing their older debts as well as borrowing for consumption reasons, while non risky borrowers apply for loans due to different reasons.  \r\n\r\n**Figure 9: Risky Borrowers**  \r\n![](https://thefaulters.github.io/Lending-Club-Default-Predictions/Images/risky_borrowerss.png)\r\n\r\n**Figure 10: Safe Borrowers**  \r\n![](https://thefaulters.github.io/Lending-Club-Default-Predictions/Images/nrisky_borrowerss.png)  \r\n  \r\nWe now select our preferred model. We will optimize - by testing distinct values for penalties and performing cross validation - 2 kinds of weighted logistic regressions, one that uses a l1 penalty (analogous to a Lasso, therefore excluding the least explanatory variables by \"shrinking\" coefficients up to 0), and the other that uses a l2 penalty (with a regulariation parameter analogous to a Ridge, that shrink coeffcients towards zero, but not to the point of excluding coefficients. We also use weighted models due to the imbalances on the response variable. Under the penalties that maximize accuracy, our weighted logistic regression with penalty l1 performs 85.8% of accurate classifications, while our weighted logistic regression with l2 penalty perfroms 75.93% of accurate classifications. However, the model with l2 penalty performs better in the ROC curve, that is, the model is more accurate under the umbalanced class. \r\n\r\nOur preferred model therefore performs 75.93% of accurate classifications, with a precision of 86.4% for class 0 and  and 17.5% for class 1. This means that we are performing an accurate classification of a very small percentage of the actually risky loans.    \r\n\r\n**Figure 11: ROC curve for sentiment analysis**  \r\n![](https://thefaulters.github.io/Lending-Club-Default-Predictions/Images/sent_ROC.png)  \r\n\r\nIt is clear from our low accuracy rating for the risky class, as well as the ROC curve above, that this model was not successful compared to our other approaches. If we were to take this further we would explore conducting other sorts of analysis on the descriptions - for example exploring libraries that might help us identify typos or bad grammar. While we could potentially draw more predictive power from such an approach, we will not pursue this approach in the context of this project.  \r\n\r\n### References\r\n\r\nCeyhan S, Shi X and Leskovec J (2011). \"Dynamics of bidding in a P2P lending service: effects of herding and predicting loan success.\" _Proceedings of the 20th International Conference on the World Wide Web (pp 547-556)._  \r\n\r\nDavis J and Goodrich M (2006). \"The relationship between Precision-Recall and ROC curves.\" _Proceedings of the 23rd International Conference on Machine learning._  \r\n\r\nHsu C, Chang C and Lin C (2016). \"A Practical Guide to Support Vector Classification.\" _National Taiwan University, Department of Computer Science._\r\n\r\nIyer R, Khwaja A, Luttmer E, Shue K (2009) “Screening in New Credit Markets: Can Individual Lenders Infer Borrower Creditworthiness in Peer-to-Peer Lending?” _Harvard Kennedy School of Government, Faculty Research Working Papers Series: RWP09-031 (September 2009)._\r\n\r\nJames G, Witten D, Hastie T and Tibshirani R (2015). \"An Introduction to Statistical Learning with Applications in R.\" _New York: Springer._\r\n\r\n[Lending Club Statistics.](https://www.lendingclub.com/info/download-data.action) _Lending Club._\r\n\r\nPandey J and Srinivasan M (2011). [\"Predicting Probability of Loan Default.\"](http://cs229.stanford.edu/proj2011/PandeySrinivasan-PredictingProbabilityOfLoanDefault.pdf) _Stanford University, CS229 Project report._ \r\n\r\nRokach L (2010). \"Ensemble-based classifiers.\" _Artificial Intelligence Review 33.1-2 (2010): 1-39_\r\n\r\nTsai K, Ramiah S and Singh S (2014). [\"Peer Lending Risk Predictor\"](http://cs229.stanford.edu/proj2014/Kevin%20Tsai,Sivagami%20Ramiah,Sudhanshu%20Singh,Peer%20Lending%20Risk%20Predictor.pdf) _Stanford University, CS229 Project Report._\r\n\r\n### Acknowledgements\r\nWe would like to thank our Teaching Fellow Qing Zhao for her guidance and support on this project. We would also like to thank the teaching staff of CS109a for their feedback and suggestions.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}